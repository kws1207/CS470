{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS492F 전산학특강<인공지능 산업 및 스마트에너지>\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-7. Practice: CIFAR10\n",
    "\n",
    "In this notebook, you are going to train convolutional neural networks in two ways:\n",
    "- From the scratch\n",
    "- From the pre-trained network \n",
    "\n",
    "Please follow the steps below to continue this practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DO NOT EDIT THE FOLLOWING LINES\n",
    "# THESE LINES ARE FOR REPRODUCIBILITY\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the CIFAR10 dataset\n",
    "In this notebook, you will use the CIFAR10 dataset which contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them.\n",
    "\n",
    "![CIFAR10](images/cifar10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cifar10.load_data()` returns four NumPy arrays:\n",
    "\n",
    "- The `train_images` and `train_labels` arrays are the training set—the data the model uses to learn.\n",
    "- The model should be tested against the test set, the `test_images`, and `test_labels` arrays.\n",
    "\n",
    "The images are 32x32 NumPy arrays, with pixel values ranging from 0 to 255. The labels are an array of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n",
    "\n",
    "| Label |    Class   |\n",
    "|-------|------------|\n",
    "|   0   | Airplane   |\n",
    "|   1   | Automobile |\n",
    "|   2   | Bird       |\n",
    "|   3   | Cat        |\n",
    "|   4   | Deer       |\n",
    "|   5   | Dog        |\n",
    "|   6   | Frog       |\n",
    "|   7   | Horse      |\n",
    "|   8   | Ship       |\n",
    "|   9   | Truck      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is mapped to a single label. Since the class names are not included with the dataset, let's store them here to use later when plotting the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess the data\n",
    "To preprocess the dataset, let's make `tf.data.Dataset` using `(train_images, train_labels)` and `(test_images, test_labels)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = tf.data.Dataset.from_tensor_slices((train_images, tf.squeeze(train_labels)))\n",
    "test_tensors = tf.data.Dataset.from_tensor_slices((test_images, tf.squeeze(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    "Preprocess the data you have loaded as follows:\n",
    "- Resize images to `(image_height, image_width)`\n",
    "- Scale values to a range of `[-1, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height, image_width = 224, 224\n",
    "\n",
    "# TODO: Preprocess the data you have loaded\n",
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that the data is in the correct format and that you're ready to build and train the network, let's display the first 25 images from the training set and display the class name below each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for index, (image, label) in enumerate(train_tensors.take(25)):\n",
    "    plt.subplot(5, 5, index + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow((image + 1) / 2, cmap=plt.cm.binary)\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[label])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's combine elements of train and test datasets into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = train_tensors.batch(32)\n",
    "test_tensors = test_tensors.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the model\n",
    "First, you are going to build a convolutional neural network that will be trained from the scratch.\n",
    "\n",
    "#### Problem 2\n",
    "Define a convolutional neural network here using `tf.keras.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a convolutional neural network to classify CIFAR10 dataset\n",
    "model_from_scratch = tf.keras.Sequential([\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3\n",
    "Load `DenseNet121` model without the top classification layer using `tf.keras.applications.DenseNet121`. Then, freeze the model to prevent it from being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load DenseNet121 model without the top classification layer\n",
    "#       Then, freeze the model to prevent it from being trained\n",
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a convolutional neural network using the loaded `DenseNet121` to classify images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a convolutional neural network using the loaded DenseNet121\n",
    "### START CODE HERE ###\n",
    "model_transferred = tf.keras.Sequential([\n",
    "\n",
    "])\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4\n",
    "Compile both models you have defined with appropriate optimizers, loss functions and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compile `model_from_scratch` with appropriate parameters\n",
    "### START CODE HERE ###\n",
    "model_from_scratch.compile(\n",
    "\n",
    ")\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compile `model_transferred` with appropriate parameters\n",
    "### START CODE HERE ###\n",
    "model_transferred.compile(\n",
    "\n",
    ")\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the model\n",
    "#### Problem 5\n",
    "Train both models at least 10 epochs using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train `model_from_scratch` at least 5 epochs using the training data\n",
    "### START CODE HERE ###\n",
    "model_from_scratch.fit(\n",
    "\n",
    ")\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train `model_transferred` at least 5 epochs using the training data\n",
    "### START CODE HERE ###\n",
    "model_transferred.fit(\n",
    "\n",
    ")\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate accuracy\n",
    "Evaluate the trained models using test dataset and print the test accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('From scratch:', model_from_scratch.evaluate(test_tensors, verbose=0))\n",
    "print('Transferred:', model_transferred.evaluate(test_tensors, verbose=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
