{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS492F 전산학특강<인공지능 산업 및 스마트에너지>\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convolutional Neural Network\n",
    "### 3-4. Case studies\n",
    "\n",
    "Constructing and training your own convolutional neural network models from scratch can be hard and a long task. A common trick used in deep learning is to use a **pre-trained** model and **fine-tune** it to the specific data it will be used for. \n",
    "\n",
    "To use the pre-trained models for our task, we will first look into several well-known CNN models. Many CNN models have been studied since the 1990s. Especially, since 2010, more advanced models have been developed  through a [ImageNet: Large scale visual recognition challenge (ILSVRC)](http://www.image-net.org/challenges/LSVRC/) in the computer vision fields such as image recognition, object detection, etc.\n",
    "\n",
    "- LeNet \n",
    "- AlexNet\n",
    "- VGG \n",
    "- MobileNet\n",
    "- Inception (GoogLeNet)\n",
    "- ResNet50 \n",
    "- Xception\n",
    "- ... more to come"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeNet\n",
    "\n",
    "![LeNet](images/lenet.png)\n",
    "\n",
    "- Yann LeCun et al. proposed a neural network architecture for handwritten and machine-printed character recognition in 1990s.\n",
    "- The first successful applications of CNN.\n",
    "- This model consists of 3 convolution layers, 2 pooling layers and 1 fully-connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet\n",
    "\n",
    "![AlexNet](images/alexnet.png)\n",
    "\n",
    "- The first work that popularized convolutional neural networks in computer vision.\n",
    "- This was submitted to the ImageNet ILSVRC challenge in 2012. \n",
    "- This network had a very similar architecture to LeNet, but was deeper, bigger, and featured convolutional layers stacked on top of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG\n",
    "##### VGG-16\n",
    "![VGG-16](images/vgg16.jpg)\n",
    "\n",
    "##### VGG-19\n",
    "![VGG-19](images/vgg19.jpg)\n",
    "\n",
    "- The runner-up in ILSVRC 2014 (VGG16)\n",
    "- Its main contribution was in showing that the depth of the network is a critical component for good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inception(v3) (GoogLeNet)\n",
    "\n",
    "![GoogLeNet](images/googlenet.png)\n",
    "\n",
    "- The winner in ILSVRC 2014\n",
    "- Its main contribution was the development of an `Inception Module` that dramatically reduced the number of parameters in the network.\n",
    "- There are also several follow-up versions to the GoogLeNet, most recently Inception-v4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet\n",
    "\n",
    "![ResNet](images/resnet.png)\n",
    "\n",
    "- The winner in ILSVRC 2015\n",
    "- It features special skip connections and a heavy use of batch normalization.\n",
    "- The architecture is also missing fully connected layers at the end of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. Image classification using the pre-trained models\n",
    "\n",
    "#### VGG16\n",
    "We can use the pre-trained CNN models mentioned above using the Keras API [tf.keras.applications](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications). (More models are available in Keras which can be found here: https://github.com/keras-team/keras-applications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily download and load the pre-trained VGG16 model using `tf.keras.applications.VGG16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download and load the pre-trained VGG16 model\n",
    "vgg16 = \n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's download a strawberry image to classify it using the pre-trained model we just have loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --output-document=\"strawberry.jpg\" https://upload.wikimedia.org/wikipedia/commons/c/ce/Bowl_of_Strawberries.jpg\n",
    "Image.open('strawberry.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed an image to the pre-trained model, we first have to apply preprocesses that the model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load and preprocess the downloaded image\n",
    "image = \n",
    "x = \n",
    "x = \n",
    "x = \n",
    "\n",
    "print('Input image shape:', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can feed the input image to the pre-trained model and get prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict the image using VGG16\n",
    "predictions = \n",
    "predictions = \n",
    "\n",
    "print(f'Top-{len(predictions)} predictions:')\n",
    "for index, prediction in enumerate(predictions):\n",
    "    print(f'{index + 1}. {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shonw in the prediction results, the VGG16 model predicted a class of the input as a _'strawberry'_ with highest confidence value (or probability), 0.9982."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to predict again with another image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --output-document=\"orange.jpg\" https://upload.wikimedia.org/wikipedia/commons/c/c4/Orange-Fruit-Pieces.jpg\n",
    "Image.open('orange.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load and preprocess the downloaded image\n",
    "image = \n",
    "x = \n",
    "x = \n",
    "x = \n",
    "\n",
    "print('Input image shape:', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict the image using VGG16\n",
    "predictions = \n",
    "predictions = \n",
    "\n",
    "print(f'Top-{len(predictions)} predictions:')\n",
    "for index, prediction in enumerate(predictions):\n",
    "    print(f'{index + 1}. {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet50\n",
    "Similar to VGG16 model, we can use RestNet50 using the Keras API. ResNet50 is so big compared to the VGG16. Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download and load the pre-trained ResNet50 model\n",
    "resnet50 = \n",
    "resnet50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict the images using ResNet50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) - please cite this paper if you use the VGG models in your work.\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - please cite this paper if you use the ResNet model in your work.\n",
    "- [Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567) - please cite this paper if you use the Inception v3 model in your work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
